{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a4s0ewmYzUq"
   },
   "outputs": [],
   "source": [
    "# This code is based on https://github.com/zoubohao/DenoisingDiffusionProbabilityModel-ddpm-/tree/57e53510e9c5932add4c09b23fb3296a207fa104, \n",
    "# which is released under the MIT licesne.\n",
    "\n",
    "# This code is also based on \"DL-Generative-Model-Assignment\" Templete.\n",
    "\n",
    "# Paper reference: \n",
    "# Classifier-free diffusion guidance: https://arxiv.org/pdf/2207.12598.pdf\n",
    "# DDPM: https://arxiv.org/pdf/2006.11239.pdf\n",
    "\n",
    "# Library used: pytorch, matplotlib, tqdm(for process bar), typing, PIL\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.datasets import STL10\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict\n",
    "cvhm\n",
    "import math\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typing\n",
      "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: typing\n",
      "  Building wheel for typing (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26306 sha256=da038ae193dbc8db3e11ee7713b909e0a11d8e3e895cdc003660d3bc64b9ed68\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/23/95/593222493ec6253200e94e4a5ee4361d12112000816d840434\n",
      "Successfully built typing\n",
      "Installing collected packages: typing\n",
      "Successfully installed typing-3.7.4.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-setting, ensure put correct device(cuba), batch_size(64), img_size(base on dataset)\n",
    "# If you are using saving/loading model or saving images function or running any evaluation-related function\n",
    "# Make sure you have folder with same name, and you have model inside. The way to get model is from programming running itself with save-function.\n",
    "\n",
    "modelConfig = {\n",
    "    \"epoch\": 300,                                     #  epoch\n",
    "    \"batch_size\": 64,                                 #  batch_size(normally is 64)\n",
    "    \"T\": 500,                                         #  Time step\n",
    "    \"channel\": 128,                                   #  number of channel\n",
    "    \"channel_mult\": [1, 2, 2, 2],                     #  channel_mult\n",
    "    \"num_res_blocks\": 2,                              #  num of res block\n",
    "    \"dropout\": 0.15,                                  #  drop out rate\n",
    "    \"lr\": 1e-4,                                       #  learing rate\n",
    "    \"multiplier\": 2.5,                                #  multiplier\n",
    "    \"beta_1\": 1e-4,                                   #  beta_1\n",
    "    \"beta_T\": 0.028,                                  #  beta_T\n",
    "    \"img_size\": 32,                                   #  img_size(base on the dataset), Cifar -> 32, STL-10 -> 48/64, the code has out of memory issue when on 96\n",
    "    \"grad_clip\": 1.,                                  #  grad_clip\n",
    "    \"device\": \"cuda\",                                 #  device(cuda)\n",
    "    \"w\": 1.8,                                         #  w\n",
    "    \"save_dir\": \"./CheckpointsCondition_cifar/\",      #  path save model               \n",
    "    \"test_load_weight\": \"ckpt_502_.pt\",               #  for evaluation use, name of the model ckpt_epoch_.pt\n",
    "    \"sampled_dir\": \"./SampledImgs/\",                  #  sample img path\n",
    "    \"sampledImgName\": None,                           #  sample img name\n",
    "    \"nrow\": 8                                         #  nrow\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(v, t, x_shape):\n",
    "    \"\"\"\n",
    "    Extract some coefficients at specified timesteps, then reshape to\n",
    "    [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n",
    "    \"\"\"\n",
    "    device = t.device\n",
    "    out = torch.gather(v, index=t, dim=0).float().to(device)\n",
    "    return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the input image, \n",
    "# noise base on predefined sequence of noise levels determined by the betas. \n",
    "\n",
    "class GaussianDiffusionTrainer(nn.Module):\n",
    "    def __init__(self, model, beta_1, beta_T, T):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.T = T\n",
    "\n",
    "        self.register_buffer(\n",
    "            'betas', torch.linspace(beta_1, beta_T, T).double())\n",
    "        alphas = 1. - self.betas\n",
    "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.register_buffer(\n",
    "            'sqrt_alphas_bar', torch.sqrt(alphas_bar))\n",
    "        self.register_buffer(\n",
    "            'sqrt_one_minus_alphas_bar', torch.sqrt(1. - alphas_bar))\n",
    "        \n",
    "    # For interpolation purpose \n",
    "    def GetAlphaOne(self, x_0):\n",
    "            \n",
    "        t = torch.randint(self.T, size=(x_0.shape[0], ), device=x_0.device)\n",
    "        noise = torch.randn_like(x_0)\n",
    "        x_t =   extract(self.sqrt_alphas_bar, t, x_0.shape) * x_0 + \\\n",
    "                extract(self.sqrt_one_minus_alphas_bar, t, x_0.shape) * noise\n",
    "        return x_t\n",
    "\n",
    "    def forward(self, x_0, labels):\n",
    "        \"\"\"\n",
    "        Algorithm 1.\n",
    "        \"\"\"\n",
    "        t = torch.randint(self.T, size=(x_0.shape[0], ), device=x_0.device)\n",
    "        noise = torch.randn_like(x_0)\n",
    "        x_t =   extract(self.sqrt_alphas_bar, t, x_0.shape) * x_0 + \\\n",
    "                extract(self.sqrt_one_minus_alphas_bar, t, x_0.shape) * noise\n",
    "        loss = F.mse_loss(self.model(x_t, t, labels), noise, reduction='none')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcTR5166d10R"
   },
   "outputs": [],
   "source": [
    "class GaussianDiffusionSampler(nn.Module):\n",
    "    def __init__(self, model, beta_1, beta_T, T, w = 0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.T = T\n",
    "        ### In the classifier free guidence paper, w is the key to control the gudience.\n",
    "        ### w = 0 and with label = 0 means no guidence.\n",
    "        ### w > 0 and label > 0 means guidence. Guidence would be stronger if w is bigger.\n",
    "        self.w = w\n",
    "\n",
    "        self.register_buffer('betas', torch.linspace(beta_1, beta_T, T).double())\n",
    "        alphas = 1. - self.betas\n",
    "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "        alphas_bar_prev = F.pad(alphas_bar, [1, 0], value=1)[:T]\n",
    "        self.register_buffer('coeff1', torch.sqrt(1. / alphas))\n",
    "        self.register_buffer('coeff2', self.coeff1 * (1. - alphas) / torch.sqrt(1. - alphas_bar))\n",
    "        self.register_buffer('posterior_var', self.betas * (1. - alphas_bar_prev) / (1. - alphas_bar))\n",
    "\n",
    "    def predict_xt_prev_mean_from_eps(self, x_t, t, eps):\n",
    "        assert x_t.shape == eps.shape\n",
    "        return extract(self.coeff1, t, x_t.shape) * x_t - extract(self.coeff2, t, x_t.shape) * eps\n",
    "\n",
    "    def p_mean_variance(self, x_t, t, labels):\n",
    "        # below: only log_variance is used in the KL computations\n",
    "        var = torch.cat([self.posterior_var[1:2], self.betas[1:]])\n",
    "        var = extract(var, t, x_t.shape)\n",
    "        eps = self.model(x_t, t, labels)\n",
    "        nonEps = self.model(x_t, t, torch.zeros_like(labels).to(labels.device))\n",
    "        eps = (1. + self.w) * eps - self.w * nonEps\n",
    "        xt_prev_mean = self.predict_xt_prev_mean_from_eps(x_t, t, eps=eps)\n",
    "        return xt_prev_mean, var\n",
    "\n",
    "    def forward(self, x_T, labels):\n",
    "        \"\"\"\n",
    "        Algorithm 2.\n",
    "        \"\"\"\n",
    "        x_t = x_T\n",
    "        for time_step in reversed(range(self.T)):\n",
    "            t = x_t.new_ones([x_T.shape[0], ], dtype=torch.long) * time_step\n",
    "            mean, var= self.p_mean_variance(x_t=x_t, t=t, labels=labels)\n",
    "            if time_step > 0:\n",
    "                noise = torch.randn_like(x_t)\n",
    "            else:\n",
    "                noise = 0\n",
    "            x_t = mean + torch.sqrt(var) * noise\n",
    "            assert torch.isnan(x_t).int().sum() == 0, \"nan in tensor.\"\n",
    "        x_0 = x_t\n",
    "        return torch.clip(x_0, -1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net\n",
    "\n",
    "def drop_connect(x, drop_ratio):\n",
    "    keep_ratio = 1.0 - drop_ratio\n",
    "    mask = torch.empty([x.shape[0], 1, 1, 1], dtype=x.dtype, device=x.device)\n",
    "    mask.bernoulli_(p=keep_ratio)\n",
    "    x.div_(keep_ratio)\n",
    "    x.mul_(mask)\n",
    "    return x\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, T, d_model, dim):\n",
    "        assert d_model % 2 == 0\n",
    "        super().__init__()\n",
    "        emb = torch.arange(0, d_model, step=2) / d_model * math.log(10000)\n",
    "        emb = torch.exp(-emb)\n",
    "        pos = torch.arange(T).float()\n",
    "        emb = pos[:, None] * emb[None, :]\n",
    "        assert list(emb.shape) == [T, d_model // 2]\n",
    "        emb = torch.stack([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        assert list(emb.shape) == [T, d_model // 2, 2]\n",
    "        emb = emb.view(T, d_model)\n",
    "\n",
    "        self.timembedding = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(emb, freeze=False),\n",
    "            nn.Linear(d_model, dim),\n",
    "            Swish(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        emb = self.timembedding(t)\n",
    "        return emb\n",
    "\n",
    "class ConditionalEmbedding(nn.Module):\n",
    "    def __init__(self, num_labels, d_model, dim):\n",
    "        assert d_model % 2 == 0\n",
    "        super().__init__()\n",
    "        self.condEmbedding = nn.Sequential(\n",
    "            nn.Embedding(num_embeddings=num_labels + 1, embedding_dim=d_model, padding_idx=0),\n",
    "            nn.Linear(d_model, dim),\n",
    "            Swish(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        emb = self.condEmbedding(t)\n",
    "        return emb\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1)\n",
    "        self.c2 = nn.Conv2d(in_ch, in_ch, 5, stride=2, padding=2)\n",
    "\n",
    "    def forward(self, x, temb, cemb):\n",
    "        x = self.c1(x) + self.c2(x)\n",
    "        return x\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.c = nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1)\n",
    "        self.t = nn.ConvTranspose2d(in_ch, in_ch, 5, 2, 2, 1)\n",
    "\n",
    "    def forward(self, x, temb, cemb):\n",
    "        _, _, H, W = x.shape\n",
    "        x = self.t(x)\n",
    "        x = self.c(x)\n",
    "        return x\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.group_norm = nn.GroupNorm(32, in_ch)\n",
    "        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.group_norm(x)\n",
    "        q = self.proj_q(h)\n",
    "        k = self.proj_k(h)\n",
    "        v = self.proj_v(h)\n",
    "\n",
    "        q = q.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        k = k.view(B, C, H * W)\n",
    "        w = torch.bmm(q, k) * (int(C) ** (-0.5))\n",
    "        assert list(w.shape) == [B, H * W, H * W]\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        v = v.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        h = torch.bmm(w, v)\n",
    "        assert list(h.shape) == [B, H * W, C]\n",
    "        h = h.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        h = self.proj(h)\n",
    "\n",
    "        return x + h\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, tdim, dropout, attn=True):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.GroupNorm(32, in_ch),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
    "        )\n",
    "        self.temb_proj = nn.Sequential(\n",
    "            Swish(),\n",
    "            nn.Linear(tdim, out_ch),\n",
    "        )\n",
    "        self.cond_proj = nn.Sequential(\n",
    "            Swish(),\n",
    "            nn.Linear(tdim, out_ch),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.GroupNorm(32, out_ch),\n",
    "            Swish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
    "        )\n",
    "        if in_ch != out_ch:\n",
    "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        if attn:\n",
    "            self.attn = AttnBlock(out_ch)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x, temb, labels):\n",
    "        h = self.block1(x)\n",
    "        h += self.temb_proj(temb)[:, :, None, None]\n",
    "        h += self.cond_proj(labels)[:, :, None, None]\n",
    "        h = self.block2(h)\n",
    "\n",
    "        h = h + self.shortcut(x)\n",
    "        h = self.attn(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, T, num_labels, ch, ch_mult, num_res_blocks, dropout):\n",
    "        super().__init__()\n",
    "        tdim = ch * 4\n",
    "        self.time_embedding = TimeEmbedding(T, ch, tdim)\n",
    "        self.cond_embedding = ConditionalEmbedding(num_labels, ch, tdim)\n",
    "        self.head = nn.Conv2d(3, ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.downblocks = nn.ModuleList()\n",
    "        chs = [ch]  # record output channel when dowmsample for upsample\n",
    "        now_ch = ch\n",
    "        for i, mult in enumerate(ch_mult):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res_blocks):\n",
    "                self.downblocks.append(ResBlock(in_ch=now_ch, out_ch=out_ch, tdim=tdim, dropout=dropout))\n",
    "                now_ch = out_ch\n",
    "                chs.append(now_ch)\n",
    "            if i != len(ch_mult) - 1:\n",
    "                self.downblocks.append(DownSample(now_ch))\n",
    "                chs.append(now_ch)\n",
    "\n",
    "        self.middleblocks = nn.ModuleList([\n",
    "            ResBlock(now_ch, now_ch, tdim, dropout, attn=True),\n",
    "            ResBlock(now_ch, now_ch, tdim, dropout, attn=False),\n",
    "        ])\n",
    "\n",
    "        self.upblocks = nn.ModuleList()\n",
    "        for i, mult in reversed(list(enumerate(ch_mult))):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res_blocks + 1):\n",
    "                self.upblocks.append(ResBlock(in_ch=chs.pop() + now_ch, out_ch=out_ch, tdim=tdim, dropout=dropout, attn=False))\n",
    "                now_ch = out_ch\n",
    "            if i != 0:\n",
    "                self.upblocks.append(UpSample(now_ch))\n",
    "        assert len(chs) == 0\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.GroupNorm(32, now_ch),\n",
    "            Swish(),\n",
    "            nn.Conv2d(now_ch, 3, 3, stride=1, padding=1)\n",
    "        )\n",
    " \n",
    "\n",
    "    def forward(self, x, t, labels):\n",
    "        # Timestep embedding\n",
    "        temb = self.time_embedding(t)\n",
    "        cemb = self.cond_embedding(labels)\n",
    "        # Downsampling\n",
    "        h = self.head(x)\n",
    "        hs = [h]\n",
    "        for layer in self.downblocks:\n",
    "            h = layer(h, temb, cemb)\n",
    "            hs.append(h)\n",
    "        # Middle\n",
    "        for layer in self.middleblocks:\n",
    "            h = layer(h, temb, cemb)\n",
    "        # Upsampling\n",
    "        for layer in self.upblocks:\n",
    "            if isinstance(layer, ResBlock):\n",
    "                h = torch.cat([h, hs.pop()], dim=1)\n",
    "            h = layer(h, temb, cemb)\n",
    "        h = self.tail(h)\n",
    "\n",
    "        assert len(hs) == 0\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A learning rate scheduler that gradually increases the learning rate during the initial training phase before reverting to a base scheduler.\n",
    "# Help to allow model converge earlier\n",
    "\n",
    "class GradualWarmupScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, multiplier, warm_epoch, after_scheduler=None):\n",
    "        self.multiplier = multiplier\n",
    "        self.total_epoch = warm_epoch\n",
    "        self.after_scheduler = after_scheduler\n",
    "        self.finished = False\n",
    "        self.last_epoch = None\n",
    "        self.base_lrs = None\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None, metrics=None):\n",
    "        if self.finished and self.after_scheduler:\n",
    "            if epoch is None:\n",
    "                self.after_scheduler.step(None)\n",
    "            else:\n",
    "                self.after_scheduler.step(epoch - self.total_epoch)\n",
    "        else:\n",
    "            return super(GradualWarmupScheduler, self).step(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation the model\n",
    "# If you wish to use this, require model trained by this program -> see Train save module function\n",
    "# If you wish not us this but still wish to see full pictures -> Uncomment specific fucntion in training process\n",
    "def eval(modelConfig: Dict):\n",
    "    device = torch.device(modelConfig[\"device\"])\n",
    "    # load model and evaluate\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # make labels for CFG\n",
    "        step = 8\n",
    "        labelList = []\n",
    "        k = 0\n",
    "        for i in range(1, modelConfig[\"batch_size\"] + 1):\n",
    "            labelList.append(torch.ones(size=[1]).long() * k)\n",
    "            if i % step == 0:\n",
    "                if k < 9:\n",
    "                    k += 1\n",
    "                    \n",
    "        labels = torch.cat(labelList, dim=0).long().to(device) + 1\n",
    "        \n",
    "        model = UNet(T=modelConfig[\"T\"], num_labels=10, ch=modelConfig[\"channel\"], ch_mult=modelConfig[\"channel_mult\"],\n",
    "                     num_res_blocks=modelConfig[\"num_res_blocks\"], dropout=modelConfig[\"dropout\"]).to(device)\n",
    "        \n",
    "        # Model path\n",
    "        ckpt = torch.load(os.path.join(\n",
    "            modelConfig[\"save_dir\"], modelConfig[\"test_load_weight\"]), map_location=device)\n",
    "        \n",
    "        model.load_state_dict(ckpt)\n",
    "        \n",
    "        print(\"model load weight done.\")\n",
    "        model.eval()\n",
    "        \n",
    "        sampler = GaussianDiffusionSampler(\n",
    "            model, modelConfig[\"beta_1\"], modelConfig[\"beta_T\"], modelConfig[\"T\"], w=modelConfig[\"w\"]).to(device)\n",
    "        \n",
    "        # Sampled from standard normal distribution\n",
    "        noisyImage = torch.randn(\n",
    "            size=[modelConfig[\"batch_size\"], 3, modelConfig[\"img_size\"], modelConfig[\"img_size\"]], device=device)\n",
    "        \n",
    "        sampledImgs = sampler(noisyImage, labels)\n",
    "        sampledImgs = sampledImgs * 0.5 + 0.5  # [0 ~ 1]\n",
    "        \n",
    "        # Uncomment if you wish to save the image to folder, change path in the dicturary.\n",
    "        # save_image(sampledImgs, os.path.join(\n",
    "        #     modelConfig[\"sampled_dir\"],  modelConfig[\"sampledImgName\"]), nrow=modelConfig[\"nrow\"])\n",
    "        \n",
    "        # show first 8 images, comment it if you don't wish to see.\n",
    "        plt.rcParams['figure.dpi'] = 100\n",
    "        plt.grid(False)\n",
    "        plt.imshow(torchvision.utils.make_grid(sampledImgs[:8]).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
    "        plt.show()\n",
    "        plt.pause(0.0001)\n",
    "        \n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        plt.rcParams['figure.dpi'] = 100\n",
    "        plt.grid(False)\n",
    "        plt.imshow(torchvision.utils.make_grid(sampledImgs).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
    "        plt.show()\n",
    "        # Uncomment if you wish to save the image to folder, remember to create folder name called \"submission\".\n",
    "        # fig.savefig(os.path.join(\"Submission\", modelConfig[\"sampledImgName\"]), dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training process\n",
    "\n",
    "def train(modelConfig: Dict):\n",
    "    device = torch.device(modelConfig[\"device\"])\n",
    "    # dataset\n",
    "    \n",
    "    # Uncomment the dataset if you wish, change the img_size in dicturary.\n",
    "    # dataset = STL10(\n",
    "    #     root='./dataset', split='train+unlabeled', download=True,\n",
    "    #     transform=transforms.Compose([\n",
    "    #         transforms.Resize((modelConfig[\"img_size\"], modelConfig[\"img_size\"])), -> img_size should be 64 or 48\n",
    "    #         transforms.ToTensor(),\n",
    "    #         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    #     ]))\n",
    "    \n",
    "    dataset = CIFAR10(\n",
    "        root='./dataset', train=True, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]))\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=modelConfig[\"batch_size\"], shuffle=False, num_workers=8, drop_last=True, pin_memory=True)\n",
    "\n",
    "    # Continue last training if you wish, remember change \"save_dir\" to correct one.\n",
    "    # state_dict = torch.load(os.path.join(modelConfig[\"save_dir\"], modelConfig[\"test_load_weight\"]))\n",
    "    \n",
    "    # model setup\n",
    "    net_model = UNet(T=modelConfig[\"T\"], num_labels=10, ch=modelConfig[\"channel\"], ch_mult=modelConfig[\"channel_mult\"],\n",
    "                     num_res_blocks=modelConfig[\"num_res_blocks\"], dropout=modelConfig[\"dropout\"]).to(device)\n",
    "    \n",
    "    # Uncomment if you need to resume the training for now.\n",
    "    # net_model.load_state_dict(state_dict)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        net_model.parameters(), lr=modelConfig[\"lr\"], weight_decay=1e-4)\n",
    "    \n",
    "    cosineScheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer=optimizer, T_max=modelConfig[\"epoch\"], eta_min=0, last_epoch=-1)\n",
    "    \n",
    "    warmUpScheduler = GradualWarmupScheduler(optimizer=optimizer, multiplier=modelConfig[\"multiplier\"],\n",
    "                                             warm_epoch=modelConfig[\"epoch\"] // 10, after_scheduler=cosineScheduler)\n",
    "    \n",
    "    trainer = GaussianDiffusionTrainer(\n",
    "        net_model, modelConfig[\"beta_1\"], modelConfig[\"beta_T\"], modelConfig[\"T\"]).to(device)\n",
    "\n",
    "    # start training\n",
    "    for e in range(modelConfig[\"epoch\"]):\n",
    "        # add process bar\n",
    "        with tqdm(dataloader, dynamic_ncols=True) as tqdmDataLoader:\n",
    "            for images, labels in tqdmDataLoader:\n",
    "                # train\n",
    "                b = images.shape[0]\n",
    "                optimizer.zero_grad()\n",
    "                x_0 = images.to(device)\n",
    "                labels = labels.to(device) + 1\n",
    "                if np.random.rand() < 0.1:\n",
    "                    labels = torch.zeros_like(labels).to(device)\n",
    "                loss = trainer(x_0, labels).sum() / b ** 2.\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    net_model.parameters(), modelConfig[\"grad_clip\"])\n",
    "                optimizer.step()\n",
    "                # process bar setting\n",
    "                tqdmDataLoader.set_postfix(ordered_dict={\n",
    "                    \"epoch\": e,\n",
    "                    \"loss: \": loss.item(),\n",
    "                    \"img shape: \": x_0.shape,\n",
    "                    \"LR\": optimizer.state_dict()['param_groups'][0][\"lr\"]\n",
    "                })\n",
    "                \n",
    "        # learning rate warm up\n",
    "        warmUpScheduler.step()\n",
    "        \n",
    "        # Uncomment to save model if you wish, remember to change \"save_dir\" to the path\n",
    "        # torch.save(net_model.state_dict(), os.path.join(\n",
    "        # modelConfig[\"save_dir\"], 'ckpt_' + str(e+) + \"_.pt\"))\n",
    "        \n",
    "        # model name:\n",
    "        # modelConfig[\"test_load_weight\"] = 'ckpt_' + str(e) + \"_.pt\"\n",
    "        # Update the sampleImgName\n",
    "        # modelConfig[\"sampledImgName\"] = \"SampledGuidenceImgs_\" + str(e) + \".png\"\n",
    "        \n",
    "        # Evaluation:\n",
    "        with torch.no_grad():\n",
    "            step = 8\n",
    "            labelList = []\n",
    "            k = 0\n",
    "            for i in range(1, modelConfig[\"batch_size\"] + 1):\n",
    "                labelList.append(torch.ones(size=[1]).long() * k)\n",
    "                if i % step == 0:\n",
    "                    if k < 9:\n",
    "                        k += 1\n",
    "            labels = torch.cat(labelList, dim=0).long().to(device) + 1\n",
    "            \n",
    "            sampler = GaussianDiffusionSampler(\n",
    "                net_model, modelConfig[\"beta_1\"], modelConfig[\"beta_T\"], modelConfig[\"T\"], w=modelConfig[\"w\"]).to(device)\n",
    "\n",
    "            noisyImage = torch.randn(\n",
    "                size=[modelConfig[\"batch_size\"], 3, modelConfig[\"img_size\"], modelConfig[\"img_size\"]], device=device)\n",
    "\n",
    "            sampledImgs = sampler(noisyImage, labels)\n",
    "            sampledImgs = sampledImgs * 0.5 + 0.5  # [0 ~ 1]\n",
    "\n",
    "            # Uncomment if you wish to save the image to folder, change path in the dicturary.\n",
    "            # save_image(sampledImgs, os.path.join(\n",
    "            #     modelConfig[\"sampled_dir\"],  modelConfig[\"sampledImgName\"]), nrow=modelConfig[\"nrow\"])\n",
    "\n",
    "            plt.rcParams['figure.dpi'] = 100\n",
    "            plt.grid(False)\n",
    "            plt.imshow(torchvision.utils.make_grid(sampledImgs[:8]).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
    "            plt.show()\n",
    "            plt.pause(0.0001)\n",
    "            \n",
    "            # Uncomment if you wish to see full picture 8x8\n",
    "            plt.rcParams['figure.dpi'] = 100\n",
    "            plt.grid(False)\n",
    "            plt.imshow(torchvision.utils.make_grid(sampledImgs).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
    "            plt.show()\n",
    "            plt.pause(0.0001)\n",
    "            \n",
    "            # Uncommment if you wish to see the interpolation result\n",
    "            z = trainer.GetAlphaOne(sampledImgs)\n",
    "\n",
    "            col_size = int(np.sqrt(modelConfig[\"batch_size\"]))\n",
    "\n",
    "            z0 = z[0:col_size].repeat(col_size,1,1,1) # z for top row\n",
    "            z1 = z[modelConfig[\"batch_size\"]-col_size:].repeat(col_size,1,1,1) # z for bottom row\n",
    "\n",
    "            t = torch.linspace(0,1,col_size).unsqueeze(1).repeat(1,col_size).view(modelConfig[\"batch_size\"],1,1,1).to(device)\n",
    "\n",
    "            lerp_z = (1-t)*z0 + t*z1 # linearly interpolate between two points in the latent space\n",
    "            lerp_g = sampler(lerp_z,labels) # sample the model at the resulting interpolated latents\n",
    "\n",
    "            lerp_g = lerp_g * 0.5 + 0.5\n",
    "\n",
    "            plt.rcParams['figure.dpi'] = 175\n",
    "            plt.grid(False)\n",
    "            plt.imshow(torchvision.utils.make_grid(lerp_g).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(modelConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJNQR-skerap"
   },
   "outputs": [],
   "source": [
    "# Interpolation process\n",
    "# If you wish to run this function you need to have path for model and model as well.\n",
    "# If you don't want to run this function but still want to see the interplation result, please uncomment Train function last part.\n",
    "\n",
    "def eval_forInterpolation(modelConfig: Dict):\n",
    "    device = torch.device(modelConfig[\"device\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # labels for CFG\n",
    "        step = 8\n",
    "        labelList = []\n",
    "        k = 0\n",
    "        for i in range(1, modelConfig[\"batch_size\"] + 1):\n",
    "            labelList.append(torch.ones(size=[1]).long() * k)\n",
    "            if i % step == 0:\n",
    "                if k < 9:\n",
    "                    k += 1\n",
    "\n",
    "        labels = torch.cat(labelList, dim=0).long().to(device) + 1\n",
    "\n",
    "        # Load model\n",
    "        model = UNet(T=modelConfig[\"T\"], num_labels=10, ch=modelConfig[\"channel\"], ch_mult=modelConfig[\"channel_mult\"],\n",
    "                     num_res_blocks=modelConfig[\"num_res_blocks\"], dropout=modelConfig[\"dropout\"]).to(device)\n",
    "\n",
    "        ckpt = torch.load(os.path.join(\n",
    "            modelConfig[\"save_dir\"], modelConfig[\"test_load_weight\"]), map_location=device)\n",
    "\n",
    "        model.load_state_dict(ckpt)\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        # Load sampler and trainer\n",
    "        sampler = GaussianDiffusionSampler(\n",
    "            model, modelConfig[\"beta_1\"], modelConfig[\"beta_T\"], modelConfig[\"T\"], w=modelConfig[\"w\"]).to(device)\n",
    "\n",
    "        trainer = GaussianDiffusionTrainer(\n",
    "            model, modelConfig[\"beta_1\"], modelConfig[\"beta_T\"], modelConfig[\"T\"]).to(device)\n",
    "\n",
    "        # Sample 8 pairs of random noise vectors\n",
    "        noise_image = torch.randn(size=[modelConfig[\"batch_size\"], 3, 32, 32], device=device)\n",
    "\n",
    "        # Get sample image\n",
    "        sample_image = sampler(noise_image,labels)\n",
    "\n",
    "        z = trainer.GetAlphaOne(sample_image)\n",
    "\n",
    "        # Follow Template\n",
    "        col_size = int(np.sqrt(modelConfig[\"batch_size\"]))\n",
    "\n",
    "        z0 = z[0:col_size].repeat(col_size,1,1,1) # z for top row\n",
    "        z1 = z[modelConfig[\"batch_size\"]-col_size:].repeat(col_size,1,1,1) # z for bottom row\n",
    "\n",
    "        t = torch.linspace(0,1,col_size).unsqueeze(1).repeat(1,col_size).view(modelConfig[\"batch_size\"],1,1,1).to(device)\n",
    "\n",
    "        lerp_z = (1-t)*z0 + t*z1 # linearly interpolate between two points in the latent space\n",
    "        lerp_g = sampler(lerp_z,labels) # sample the model at the resulting interpolated latents\n",
    "\n",
    "        lerp_g = lerp_g * 0.5 + 0.5\n",
    "\n",
    "        # plot the result\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        plt.rcParams['figure.dpi'] = 175\n",
    "        plt.grid(False)\n",
    "        plt.imshow(torchvision.utils.make_grid(lerp_g).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
    "        plt.show()\n",
    "        \n",
    "        # uncomment it if you wish to save\n",
    "        # fig.savefig(os.path.join(\"Submission\", \"CifarInter.png\"), dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2QuuG4qeP5l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOwBkalteaR8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbwH3HDZexhy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuGn3d6DeyXK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
